{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_applications.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbhtVBfGpOvDW7VOYyj+ik",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipeserna/holbertonschool-machine_learning/blob/master/supervised_learning/0x12-transformer_apps/transformer_applications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WL2eWN4K2k8"
      },
      "source": [
        "# 3-dataset.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Class that loads and preps a dataset for machine translation.\n",
        "Portugese-English translation dataset.\n",
        "Approximately 50000 training examples, 1100 validation examples,\n",
        "and 2000 test examples.\n",
        "https://www.programmersought.com/article/38506277799/\n",
        "\"\"\"\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "class Dataset():\n",
        "    \"\"\"\n",
        "    Loads and preps a dataset for machine translation\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, max_len):\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "        \"\"\"\n",
        "        examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
        "                                       with_info=True,\n",
        "                                       as_supervised=True)\n",
        "\n",
        "        self.metadata = metadata\n",
        "\n",
        "        self.data_train = examples['train']\n",
        "        self.data_valid = examples['validation']\n",
        "\n",
        "        tokenizer_pt, tokenizer_en = self.tokenize_dataset(self.data_train)\n",
        "        # Portuguese tokenizer created from the training set\n",
        "        self.tokenizer_pt = tokenizer_pt\n",
        "        # English tokenizer created from the training set\n",
        "        self.tokenizer_en = tokenizer_en\n",
        "\n",
        "        # tokenizing the examples\n",
        "        # Dataset.map Maps map_func across the elements of this dataset.\n",
        "        self.data_train = self.data_train.map(self.tf_encode)\n",
        "\n",
        "        # tokenizing the examples\n",
        "        self.data_valid = self.data_valid.map(self.tf_encode)\n",
        "\n",
        "        def filter_max_length(x, y, max_length=max_len):\n",
        "            \"\"\"\n",
        "            function for .filter() method\n",
        "            \"\"\"\n",
        "            return tf.logical_and(tf.size(x) <= max_length,\n",
        "                                  tf.size(y) <= max_length)\n",
        "        \n",
        "        # Update data_train attribute\n",
        "        self.data_train = self.data_train.filter(filter_max_length)\n",
        "        self.data_train = self.data_train.cache()\n",
        "\n",
        "        train_dataset_size = self.metadata.splits['train'].num_examples\n",
        "\n",
        "        self.data_train = self.data_train.shuffle(train_dataset_size)\n",
        "        padded_shapes = ([None], [None])\n",
        "        self.data_train = self.data_train.padded_batch(batch_size,\n",
        "                                                       padded_shapes=padded_shapes)\n",
        "        \n",
        "        self.data_train = self.data_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        # Update data_valid attribute\n",
        "        self.data_valid = self.data_valid.filter(filter_max_length)\n",
        "        padded_shapes = ([None], [None])\n",
        "        self.data_valid = self.data_valid.padded_batch(batch_size,\n",
        "                                                       padded_shapes=padded_shapes)\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        \"\"\"\n",
        "        Creates sub-word tokenizers for our dataset\n",
        "        Returns: tokenizer_pt, tokenizer_en\n",
        "        \"\"\"\n",
        "        tokenizer_pt = \\\n",
        "            tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "                (pt.numpy() for pt, en in data), target_vocab_size=2**15)\n",
        "\n",
        "        tokenizer_en = \\\n",
        "            tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "                (en.numpy() for pt, en in data), target_vocab_size=2**15)\n",
        "\n",
        "        return tokenizer_pt, tokenizer_en\n",
        "\n",
        "    def encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        Encodes a translation into tokens.\n",
        "        Returns: pt_tokens, en_tokens\n",
        "        \"\"\"\n",
        "        pt_tokens = [self.tokenizer_pt.vocab_size] + self.tokenizer_pt.encode(\n",
        "            pt.numpy()) + [self.tokenizer_pt.vocab_size + 1]\n",
        "\n",
        "        en_tokens = [self.tokenizer_en.vocab_size] + self.tokenizer_en.encode(\n",
        "            en.numpy()) + [self.tokenizer_en.vocab_size + 1]\n",
        "\n",
        "        return pt_tokens, en_tokens\n",
        "\n",
        "    def tf_encode(self, pt, en):\n",
        "        \"\"\"\n",
        "        tf wrapper for the 'encode' instance method to be used with map()\n",
        "        \"\"\"\n",
        "        result_pt, result_en = tf.py_function(func=self.encode, inp=[pt, en],\n",
        "                                              Tout=[tf.int64, tf.int64])\n",
        "        # None allows any value\n",
        "        result_pt.set_shape([None])\n",
        "        result_en.set_shape([None])\n",
        "\n",
        "        return result_pt, result_en"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pWN1afsmPI1"
      },
      "source": [
        "# 4-create_masks.py\n",
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    It ensures that the model does not treat padding as the input.\n",
        "    \"\"\"\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    # (batch_size, 1, 1, seq_len)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQMUR0FpngwE"
      },
      "source": [
        "# 4-create_masks.py\n",
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    Mask used to mask the future tokens in a sequence.\n",
        "    \"\"\"\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    # (seq_len, seq_len)\n",
        "    return mask"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmx-vqUEE5xd"
      },
      "source": [
        "# 4-create_masks.py\n",
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "def create_masks(inputs, target):\n",
        "    \"\"\"\n",
        "    Creates all masks for training/validation.\n",
        "    Returns: encoder_mask, combined_mask, decoder_mask\n",
        "    \"\"\"\n",
        "    # Encoder padding mask\n",
        "    encoder_mask = create_padding_mask(inputs)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    decoder_mask = create_padding_mask(inputs)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(target)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(target)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return encoder_mask, combined_mask, decoder_mask"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMMeqhDuEq_a"
      },
      "source": [
        "# 5-transformer.py\n",
        "\"\"\"\n",
        "Transformer from project 0x11. Attention.\n",
        "You may need to make slight adjustments to this model\n",
        "to get it to functionally train.\n",
        "https://www.tensorflow.org/text/tutorials/transformer\n",
        "\"\"\"\n",
        "# import tensorflow.compat.v2 as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def positional_encoding(max_seq_len, dm):\n",
        "    \"\"\"\n",
        "    Calculates the positional encoding for a transformer.\n",
        "    Returns: a numpy.ndarray of shape (max_seq_len, dm)\n",
        "    containing the positional encoding vectors\n",
        "    \"\"\"\n",
        "    PE = np.zeros([max_seq_len, dm])\n",
        "\n",
        "    for pos in range(max_seq_len):\n",
        "        for i in range(0, dm, 2):\n",
        "            # sin to even indices\n",
        "            PE[pos, i] = np.sin(pos / (10000 ** (i / dm)))\n",
        "            # cos to odd indices\n",
        "            PE[pos, i + 1] = np.cos(pos / (10000 ** (i / dm)))\n",
        "\n",
        "    return PE"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zmY2EJr3yX3"
      },
      "source": [
        "# 5-transformer.py\n",
        "def sdp_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Calculates the scaled dot product attention.\n",
        "    Returns: output, weights\n",
        "    \"\"\"\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_QK / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(weights, V)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, weights"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AejoqyOf3_9T"
      },
      "source": [
        "# 5-transformer.py\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Performs multi head attention\n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h):\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Number of heads\n",
        "        self.h = h\n",
        "        # Dimensionality of the model\n",
        "        self.dm = dm\n",
        "        # Depth of each attention head\n",
        "        self.depth = dm // h\n",
        "        # Dense layer used to generate the query matrix\n",
        "        self.Wq = tf.keras.layers.Dense(units=dm)\n",
        "        # Dense layer used to generate the key matrix\n",
        "        self.Wk = tf.keras.layers.Dense(units=dm)\n",
        "        # Dense layer used to generate the value matrix\n",
        "        self.Wv = tf.keras.layers.Dense(units=dm)\n",
        "        # Dense layer used to generate the attention output\n",
        "        self.linear = tf.keras.layers.Dense(units=dm)\n",
        "\n",
        "    def split_heads(self, x, batch):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (h, depth).\n",
        "        Transpose the result such that the shape is\n",
        "        (batch_size, h, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch, -1, self.h, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, Q, K, V, mask):\n",
        "        \"\"\"\n",
        "        Returns: output, weights\n",
        "        \"\"\"\n",
        "        batch = tf.shape(Q)[0]\n",
        "        # Helping Kelsie\n",
        "        # batch = Q.get_shape().as_list()[0]\n",
        "\n",
        "        # (batch, seq_len_q, dk)\n",
        "        Q = self.Wq(Q)\n",
        "        # (batch, seq_len_v, dk)\n",
        "        K = self.Wk(K)\n",
        "        # (batch, seq_len_v, dv)\n",
        "        V = self.Wv(V)\n",
        "\n",
        "        # (batch, h, seq_len_q, depth)\n",
        "        Q = self.split_heads(Q, batch)\n",
        "        # (batch, h, seq_len_k, depth)\n",
        "        K = self.split_heads(K, batch)\n",
        "        # (batch, h, seq_len_v, depth)\n",
        "        V = self.split_heads(V, batch)\n",
        "\n",
        "        # scaled_attention.shape == (batch, h, seq_len_q, depth)\n",
        "        # weights.shape == (batch, h, seq_len_q, seq_len_k)\n",
        "        scaled_attention, weights = sdp_attention(Q, K, V, mask)\n",
        "\n",
        "        # (batch, seq_len_q, h, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # (batch, seq_len_q, dm)\n",
        "        concat_attention = \\\n",
        "            tf.reshape(scaled_attention, (batch, -1, self.dm))\n",
        "\n",
        "        # (batch, seq_len_q, dm)\n",
        "        output = self.linear(concat_attention)\n",
        "\n",
        "        return output, weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gytRVDXT4KUe"
      },
      "source": [
        "# 5-transformer.py\n",
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Creates an encoder block for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h, hidden, drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # MultiHeadAttention layer\n",
        "        self.mha = MultiHeadAttention(dm, h)\n",
        "        # the hidden dense layer with hidden units and relu activation\n",
        "        self.dense_hidden = tf.keras.layers.Dense(units=hidden,\n",
        "                                                  activation='relu')\n",
        "        # the output dense layer with dm units\n",
        "        self.dense_output = tf.keras.layers.Dense(units=dm)\n",
        "        # the first layer norm layer, with epsilon=1e-6\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        # the second layer norm layer, with epsilon=1e-6\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        # the first dropout layer\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_rate)\n",
        "        # the second dropout layer\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        \"\"\"\n",
        "        Returns: a tensor of shape (batch, input_seq_len, dm)\n",
        "        containing the block’s output\n",
        "        \"\"\"\n",
        "        # (batch, input_seq_len, dm)\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        # (batch, input_seq_len, dm)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # (batch, input_seq_len, dm)\n",
        "        ffn_output = self.dense_hidden(out1)\n",
        "        ffn_output = self.dense_output(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        # (batch, input_seq_len, dm)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTfkRU-Z4TLs"
      },
      "source": [
        "# 5-transformer.py\n",
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Creates a decoder block for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, dm, h, hidden, drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # the first MultiHeadAttention layer\n",
        "        self.mha1 = MultiHeadAttention(dm, h)\n",
        "        # the second MultiHeadAttention layer\n",
        "        self.mha2 = MultiHeadAttention(dm, h)\n",
        "        # the hidden dense layer with hidden units and relu activation\n",
        "        self.dense_hidden = tf.keras.layers.Dense(units=hidden,\n",
        "                                                  activation='relu')\n",
        "        # the output dense layer with dm units\n",
        "        self.dense_output = tf.keras.layers.Dense(units=dm)\n",
        "        # the first layer norm layer, with epsilon=1e-6\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        # the second layer norm layer, with epsilon=1e-6\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        # the third layer norm layer, with epsilon=1e-6\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        # the first dropout layer\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate=drop_rate)\n",
        "        # the second dropout layer\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate=drop_rate)\n",
        "        # the third dropout layer\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate=drop_rate)\n",
        "\n",
        "    def call(self, x, encoder_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Returns: a tensor of shape (batch, target_seq_len, dm)\n",
        "        containing the block’s output\n",
        "        \"\"\"\n",
        "        # encoder_output.shape == (batch, input_seq_len, dm)\n",
        "\n",
        "        # (batch, target_seq_len, dm)\n",
        "        attn1, _ = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        # (batch, target_seq_len, dm)\n",
        "        attn2, _ = self.mha2(out1, encoder_output, encoder_output,\n",
        "                             padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        # (batch, target_seq_len, dm)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        # (batch, target_seq_len, dm)\n",
        "        ffn_output = self.dense_hidden(out2)\n",
        "        ffn_output = self.dense_output(ffn_output)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        # (batch, target_seq_len, dm)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3GApw3c4nQN"
      },
      "source": [
        "# 5-transformer.py\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Creates the encoder for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden, input_vocab, max_seq_len,\n",
        "                 drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # number of blocks in the encoder\n",
        "        self.N = N\n",
        "        # dimensionality of the model\n",
        "        self.dm = dm\n",
        "        # the embedding layer for the inputs\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=input_vocab,\n",
        "                                                   output_dim=dm)\n",
        "        # numpy.ndarray (max_seq_len, dm) containing the positional encodings\n",
        "        self.positional_encoding = positional_encoding(max_seq_len, self.dm)\n",
        "        self.blocks = [EncoderBlock(dm, h, hidden, drop_rate)\n",
        "                       for _ in range(N)]\n",
        "        # the dropout layer, to be applied to the positional encodings\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=drop_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Returns: a tensor of shape (batch, input_seq_len, dm)\n",
        "        containing the encoder output\n",
        "        \"\"\"\n",
        "        # input_seq_len = tf.shape(x)[1]\n",
        "        # TypeError: slice indices must be integers\n",
        "        # or None or have an __index__ method\n",
        "        input_seq_len = x.shape[1]\n",
        "\n",
        "        # Compute the embeddings\n",
        "        # (batch, input_seq_len, dm)\n",
        "        embeddings = self.embedding(x)\n",
        "        # Scale the embeddings\n",
        "        embeddings *= tf.math.sqrt(tf.cast(self.dm, tf.float32))\n",
        "        # Sum the positional encodings with the embeddings\n",
        "        embeddings += self.positional_encoding[:input_seq_len]\n",
        "\n",
        "        output = self.dropout(embeddings, training=training)\n",
        "\n",
        "        for i in range(self.N):\n",
        "            output = self.blocks[i](output, training, mask)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqM9dRf64u_t"
      },
      "source": [
        "# 5-transformer.py\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Creates the decoder for a transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden, target_vocab, max_seq_len,\n",
        "                 drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # number of blocks in the decoder\n",
        "        self.N = N\n",
        "        # dimensionality of the model\n",
        "        self.dm = dm\n",
        "        # the embedding layer for the targets\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=target_vocab,\n",
        "                                                   output_dim=dm)\n",
        "        # numpy.ndarray (max_seq_len, dm) containing the positional encodings\n",
        "        self.positional_encoding = positional_encoding(max_seq_len, dm)\n",
        "        # a list of length N containing all of the DecoderBlock‘s\n",
        "        self.blocks = [DecoderBlock(dm, h, hidden, drop_rate)\n",
        "                       for _ in range(N)]\n",
        "        # the dropout layer, to be applied to the positional encodings\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=drop_rate)\n",
        "\n",
        "    def call(self, x, encoder_output, training, look_ahead_mask,\n",
        "             padding_mask):\n",
        "        \"\"\"\n",
        "        Returns: a tensor of shape (batch, target_seq_len, dm)\n",
        "        containing the decoder output\n",
        "        \"\"\"\n",
        "        target_seq_len = x.shape[1]\n",
        "\n",
        "        # Compute the embeddings\n",
        "        # (batch, target_seq_len, dm)\n",
        "        embeddings = self.embedding(x)\n",
        "        # Scale the embeddings\n",
        "        embeddings *= tf.math.sqrt(tf.cast(self.dm, tf.float32))\n",
        "        # Sum the positional encodings with the embeddings\n",
        "        embeddings += self.positional_encoding[:target_seq_len]\n",
        "\n",
        "        output = self.dropout(embeddings, training=training)\n",
        "\n",
        "        for i in range(self.N):\n",
        "            output = self.blocks[i](output, encoder_output, training,\n",
        "                                    look_ahead_mask, padding_mask)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IazDyE0417c"
      },
      "source": [
        "# 5-transformer.py\n",
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Creates a transformer network\n",
        "    \"\"\"\n",
        "    def __init__(self, N, dm, h, hidden, input_vocab, target_vocab,\n",
        "                 max_seq_input, max_seq_target, drop_rate=0.1):\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # the encoder layer\n",
        "        self.encoder = Encoder(N, dm, h, hidden, input_vocab, max_seq_input,\n",
        "                               drop_rate)\n",
        "        # the decoder layer\n",
        "        self.decoder = Decoder(N, dm, h, hidden, target_vocab, max_seq_target,\n",
        "                               drop_rate)\n",
        "        # a final Dense layer with target_vocab units\n",
        "        self.linear = tf.keras.layers.Dense(units=target_vocab)\n",
        "\n",
        "    def call(self, inputs, target, training,\n",
        "             encoder_mask, look_ahead_mask, decoder_mask):\n",
        "        \"\"\"\n",
        "        Returns: a tensor of shape (batch, target_seq_len, target_vocab)\n",
        "        containing the transformer output\n",
        "        \"\"\"\n",
        "        # (batch, input_seq_len, dm)\n",
        "        enc_output = self.encoder(inputs, training, encoder_mask)\n",
        "\n",
        "        # dec_output.shape == (batch, target_seq_len, dm)\n",
        "        # Error: dec_output, _ = self.decoder(target...)\n",
        "        dec_output = self.decoder(target, enc_output, training,\n",
        "                                  look_ahead_mask, decoder_mask)\n",
        "\n",
        "        final_output = self.linear(dec_output)\n",
        "\n",
        "        return final_output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0yIK0jW486s"
      },
      "source": [
        "# 5-train.py\n",
        "\"\"\"\n",
        "Creates and trains a transformer model\n",
        "for machine translation of Portuguese to English\n",
        "using our previously created dataset.\n",
        "https://www.tensorflow.org/text/tutorials/transformer\n",
        "\"\"\"\n",
        "# import tensorflow.compat.v2 as tf\n",
        "# Dataset = __import__('3-dataset').Dataset\n",
        "# create_masks = __import__('4-create_masks').create_masks\n",
        "# Transformer = __import__('5-transformer').Transformer\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"\n",
        "    CustomSchedule class\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        \"\"\"\n",
        "        Class constructor\n",
        "        \"\"\"\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        \"\"\"\n",
        "        call function\n",
        "        \"\"\"\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObAsZZbXlpLH"
      },
      "source": [
        "# 5-train.py\n",
        "def train_transformer(N, dm, h, hidden, max_len, batch_size, epochs):\n",
        "    \"\"\"\n",
        "    Returns the trained model\n",
        "    \"\"\"\n",
        "    data = Dataset(batch_size, max_len)\n",
        "\n",
        "    learning_rate = CustomSchedule(dm)\n",
        "\n",
        "    optimizer = \\\n",
        "        tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                 epsilon=1e-9)\n",
        "\n",
        "    # sparse categorical crossentropy\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    \n",
        "    def loss_function(real, pred):\n",
        "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "        loss_ = loss_object(real, pred)\n",
        "\n",
        "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "        loss_ *= mask\n",
        "\n",
        "        return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        name='train_accuracy')\n",
        "\n",
        "    input_vocab_size = data.tokenizer_pt.vocab_size + 2\n",
        "    target_vocab_size = data.tokenizer_en.vocab_size + 2\n",
        "\n",
        "    transformer = \\\n",
        "        Transformer(N=N, dm=dm, h=h,\n",
        "                    hidden=hidden,\n",
        "                    input_vocab=input_vocab_size,\n",
        "                    target_vocab=target_vocab_size,\n",
        "                    max_seq_input=max_len,\n",
        "                    max_seq_target=max_len)\n",
        "        \n",
        "    def train_step(inp, tar):\n",
        "        tar_inp = tar[:, :-1]\n",
        "        tar_real = tar[:, 1:]\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
        "            create_masks(inp, tar_inp)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(inp, tar_inp,\n",
        "                                      True,\n",
        "                                      enc_padding_mask,\n",
        "                                      combined_mask,\n",
        "                                      dec_padding_mask)\n",
        "            loss = loss_function(tar_real, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients,\n",
        "                                      transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(tar_real, predictions)\n",
        "\n",
        "    # training\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        train_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "        \n",
        "        for (batch, (inp, tar)) in enumerate(data.data_train):\n",
        "            train_step(inp, tar)\n",
        "\n",
        "            if batch % 50 == 0:\n",
        "                print('Epoch {}, batch {}: loss {} accuracy {}'.format(\n",
        "                    epoch + 1, batch,\n",
        "                    train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "        print('Epoch {}: loss {} accuracy {}'.\n",
        "              format(epoch + 1,\n",
        "                     train_loss.result(),\n",
        "                     train_accuracy.result()))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSv757WY5cQi",
        "outputId": "dfb060c0-de8b-4625-d7c7-dce44624e7da"
      },
      "source": [
        "# 5-main.py\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.compat.v1.set_random_seed(0)\n",
        "transformer = train_transformer(4, 128, 8, 512, 32, 40, 2)\n",
        "print(type(transformer))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, batch 0: loss 10.247076034545898 accuracy 0.0\n",
            "Epoch 1, batch 50: loss 10.212627410888672 accuracy 0.0012723066611215472\n",
            "Epoch 1, batch 100: loss 10.133011817932129 accuracy 0.011149213649332523\n",
            "Epoch 1, batch 150: loss 10.015901565551758 accuracy 0.015223746187984943\n",
            "Epoch 1, batch 200: loss 9.859914779663086 accuracy 0.016006920486688614\n",
            "Epoch 1, batch 250: loss 9.667799949645996 accuracy 0.018720442429184914\n",
            "Epoch 1, batch 300: loss 9.447184562683105 accuracy 0.02275059185922146\n",
            "Epoch 1, batch 350: loss 9.20382308959961 accuracy 0.0269039086997509\n",
            "Epoch 1, batch 400: loss 8.951213836669922 accuracy 0.03126164898276329\n",
            "Epoch 1, batch 450: loss 8.70673656463623 accuracy 0.03480573371052742\n",
            "Epoch 1, batch 500: loss 8.490663528442383 accuracy 0.03764767944812775\n",
            "Epoch 1, batch 550: loss 8.303106307983398 accuracy 0.04029451683163643\n",
            "Epoch 1, batch 600: loss 8.133264541625977 accuracy 0.043098270893096924\n",
            "Epoch 1, batch 650: loss 7.976191997528076 accuracy 0.04650436341762543\n",
            "Epoch 1, batch 700: loss 7.828473091125488 accuracy 0.05041373521089554\n",
            "Epoch 1, batch 750: loss 7.690517425537109 accuracy 0.05429121479392052\n",
            "Epoch 1, batch 800: loss 7.564997673034668 accuracy 0.05786693096160889\n",
            "Epoch 1, batch 850: loss 7.4460906982421875 accuracy 0.061413370072841644\n",
            "Epoch 1, batch 900: loss 7.336852073669434 accuracy 0.06493813544511795\n",
            "Epoch 1, batch 950: loss 7.234644412994385 accuracy 0.06827620416879654\n",
            "Epoch 1, batch 1000: loss 7.139781951904297 accuracy 0.07133353501558304\n",
            "Epoch 1, batch 1050: loss 7.050944805145264 accuracy 0.07444331049919128\n",
            "Epoch 1: loss 7.0120062828063965 accuracy 0.07579459995031357\n",
            "Epoch 2, batch 0: loss 4.994960784912109 accuracy 0.1517857164144516\n",
            "Epoch 2, batch 50: loss 5.125478267669678 accuracy 0.14275118708610535\n",
            "Epoch 2, batch 100: loss 5.107175350189209 accuracy 0.1434410959482193\n",
            "Epoch 2, batch 150: loss 5.063868045806885 accuracy 0.14457184076309204\n",
            "Epoch 2, batch 200: loss 5.046414375305176 accuracy 0.14560271799564362\n",
            "Epoch 2, batch 250: loss 5.035721302032471 accuracy 0.14684271812438965\n",
            "Epoch 2, batch 300: loss 5.017922401428223 accuracy 0.14771030843257904\n",
            "Epoch 2, batch 350: loss 5.002674102783203 accuracy 0.14873823523521423\n",
            "Epoch 2, batch 400: loss 4.98394250869751 accuracy 0.1497407853603363\n",
            "Epoch 2, batch 450: loss 4.971508502960205 accuracy 0.1504417061805725\n",
            "Epoch 2, batch 500: loss 4.9588141441345215 accuracy 0.15151603519916534\n",
            "Epoch 2, batch 550: loss 4.947005271911621 accuracy 0.15221115946769714\n",
            "Epoch 2, batch 600: loss 4.9338531494140625 accuracy 0.15317651629447937\n",
            "Epoch 2, batch 650: loss 4.918970584869385 accuracy 0.15412548184394836\n",
            "Epoch 2, batch 700: loss 4.907270908355713 accuracy 0.15474769473075867\n",
            "Epoch 2, batch 750: loss 4.892567157745361 accuracy 0.15563705563545227\n",
            "Epoch 2, batch 800: loss 4.881280422210693 accuracy 0.15645363926887512\n",
            "Epoch 2, batch 850: loss 4.869274139404297 accuracy 0.15729716420173645\n",
            "Epoch 2, batch 900: loss 4.8592963218688965 accuracy 0.15794682502746582\n",
            "Epoch 2, batch 950: loss 4.848429203033447 accuracy 0.15874212980270386\n",
            "Epoch 2, batch 1000: loss 4.838718414306641 accuracy 0.1592472344636917\n",
            "Epoch 2, batch 1050: loss 4.828134536743164 accuracy 0.15991877019405365\n",
            "Epoch 2: loss 4.823808193206787 accuracy 0.1602649986743927\n",
            "<class 'NoneType'>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}